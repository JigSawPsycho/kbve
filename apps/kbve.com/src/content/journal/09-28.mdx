---
title: 'September: 28th'
category: Daily
date: 2024-09-28 12:00:00
client: Self
unsplash: 1693550345119-ec1c806d74c2
img: https://images.unsplash.com/photo-1693550345119-ec1c806d74c2?crop=entropy&cs=srgb&fm=jpg&ixid=MnwzNjM5Nzd8MHwxfHJhbmRvbXx8fHx8fHx8fDE2ODE3NDg2ODY&ixlib=rb-4.0.3&q=85
description: September 28th. 
tags:
  - daily
---

import { Adsense, Tasks } from '@kbve/astropad';

## 2024

**File**

The issue that we are currently facing is the installation of a custom file into the container.

Quick commands to help debug this situation:

```shell

# Get Supabase Pods
kubectl get pods -n supabase

# Get Logs 
kubectl logs supabase-release-supabase-db-1-initdb-7djvz -n supabase

# Describe
kubectl describe pod supabase-release-supabase-db-1-initdb-2wwtr -n supabase

```

Okay to load the pgsodium key / shell, we can try to setup a sidecar with the information.
Actually since its the initDB, we can get away with using the environment because the whole container would get destroyed once its done with the init?
We would load the sidecar then through another deployment which would be the recovery? Hmm, there are ways to make this work.

Here is the pod template that we used to init the container:

```yaml 

  podTemplate:
    spec:
      initContainers:
        - name: pgsodium-getkey-init
          image: busybox
          command: ["/bin/sh", "-c", "kubectl get secret secret-pgsodium-key -n supabase -o jsonpath='{.data.pgsodium_key}' | base64 --decode > /pgsodium/keyfile"]
          volumeMounts:
            - name: pgsodium-keyfile-volume
              mountPath: /pgsodium
      containers:
        - name: main-container
          image: {{ .Values.db.image.repository }}:{{ .Values.db.image.tag }}
          volumeMounts:
            - name: pgsodium-keyfile-volume
              mountPath: /pgsodium
      volumes:
        - name: pgsodium-keyfile-volume
          emptyDir: {}

```

However, for now, we will just load the key in via the environmental variable.

**SegFault**

Looks like after disabling the pgsodium, we had to wrap back around and disable the kilobase extension as well because I believe I might have made an error with how I went to set up the extension.
I will look into the extension at another time, as there are just too many things on the plate and if worse comes to worse, we can just move the extension out of the postgres and into our own structure.
This is a tough moment but its okay, I know when to move on and there are a decent amount of other things that are on our plate to deal with.

- repo : `https://github.com/KBVE/kbve.git`
- path: `/migrations/kube/charts/kilobase`

There are 4 more core SQL files that we need to add into the configuration.
Well there were 5 total, but we got the websockets to execute without any issues.

Just to recap the 5 total:

- jwt. - done using parameters.
- logs. - giving it to the `supabase_admin`
- realtime.
- roles.
- webhooks. - works fine for now.

We got the webhooks out of the way and we can just pass in the jwt directly through the parameters.

Now there is another error that we need to look through, which was the `supabase-secret-db`, but I was under the impression that we already did this through the sealed secrets, hmm.
Okay the other three core SQLs files that we need are these:


```sql

    -- 99-logs.sql: |
    \set pguser `echo "$POSTGRES_USER"`

    create schema if not exists _analytics;
    alter schema _analytics owner to :pguser;
    -- 99-realtime.sql: |
    \set pguser `echo "$POSTGRES_USER"`

    create schema if not exists _realtime;
    alter schema _realtime owner to :pguser;
    -- 99-roles.sql: |
    -- NOTE: change to your own passwords for production environments
    \set pgpass `echo "$POSTGRES_PASSWORD"`

    ALTER USER authenticator WITH PASSWORD :'pgpass';
    ALTER USER pgbouncer WITH PASSWORD :'pgpass';
    ALTER USER supabase_auth_admin WITH PASSWORD :'pgpass';
    ALTER USER supabase_functions_admin WITH PASSWORD :'pgpass';
    ALTER USER supabase_storage_admin WITH PASSWORD :'pgpass';

```

Now of these three core files, we can start with the small.
For the log, we looked back at the supabase docker swarm and it looks like we set it to the `supabase_admin`, so lets go ahead and add that into the configmap.

The best move we can make right now would be to shut the whole thing down and restart the cluster.